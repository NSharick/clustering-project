{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e8cb8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2d9fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a64465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import f_regression, SelectKBest, RFE\n",
    "from scipy import stats\n",
    "import math\n",
    "from wrangle_zillow import acquire_zillow\n",
    "from wrangle_zillow import missing_col_values\n",
    "from wrangle_zillow import missing_row_values\n",
    "from wrangle_zillow import single_unit_properties\n",
    "from wrangle_zillow import data_prep\n",
    "from wrangle_zillow import split_data\n",
    "from wrangle_zillow import remove_outliers\n",
    "from wrangle_zillow import split_by_region\n",
    "from wrangle_zillow import lr_model_prep\n",
    "from wrangle_zillow import cluster_model_prep\n",
    "\n",
    "\n",
    "from sklearn.linear_model import TweedieRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc21106",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire_zillow()\n",
    "df = single_unit_properties(df)\n",
    "remove_cols = ['buildingqualitytypeid', 'heatingorsystemtypeid', 'propertyzoningdesc', 'heatingorsystemdesc']\n",
    "df = data_prep(df, cols_to_remove=remove_cols, prop_required_column=.5, prop_required_row=.75)\n",
    "train, validate, test = split_data(df)\n",
    "out_columns = ['bedroomcnt', 'bathroomcnt', 'calculatedfinishedsquarefeet',]\n",
    "train = remove_outliers(train, 1.5, out_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b68589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"calculatedfinishedsquarefeet\", y=\"logerror\", data=train)\n",
    "plt.xlabel(\"home sqr foot\")\n",
    "plt.ylabel(\"log error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8c2034",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign an alpha value\n",
    "alpha = 0.05\n",
    "#assign the x and y variables from the train dataset\n",
    "x = train.calculatedfinishedsquarefeet\n",
    "y = train.logerror\n",
    "#run a pearson r correlation test\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "#output the results of the test\n",
    "print(f'Correlation Coefficient = {corr} P-Value = {p}')\n",
    "print('-----------------------------------')\n",
    "if p < alpha:\n",
    "    print('We reject the null hypothesis')\n",
    "else:\n",
    "    print('We do not reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae4152",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba85e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"bathroomcnt\", y=\"logerror\", data=train)\n",
    "plt.xlabel(\"baths\")\n",
    "plt.ylabel(\"log error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046faad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign an alpha value\n",
    "alpha = 0.05\n",
    "#assign the x and y variables from the train dataset\n",
    "x = train.bathroomcnt\n",
    "y = train.logerror\n",
    "#run a pearson r correlation test\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "#output the results of the test\n",
    "print(f'Correlation Coefficient = {corr} P-Value = {p}')\n",
    "print('-----------------------------------')\n",
    "if p < alpha:\n",
    "    print('We reject the null hypothesis')\n",
    "else:\n",
    "    print('We do not reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebf313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf32b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x=\"bedroomcnt\", y=\"logerror\", data=train)\n",
    "plt.xlabel(\"beds\")\n",
    "plt.ylabel(\"log error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9d9b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign an alpha value\n",
    "alpha = 0.05\n",
    "#assign the x and y variables from the train dataset\n",
    "x = train.bedroomcnt\n",
    "y = train.logerror\n",
    "#run a pearson r correlation test\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "#output the results of the test\n",
    "print(f'Correlation Coefficient = {corr} P-Value = {p}')\n",
    "print('-----------------------------------')\n",
    "if p < alpha:\n",
    "    print('We reject the null hypothesis')\n",
    "else:\n",
    "    print('We do not reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53a6929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11de3324",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"latitude\", y=\"logerror\", data=train)\n",
    "plt.xlabel(\"latitude\")\n",
    "plt.ylabel(\"log error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1882fcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign an alpha value\n",
    "alpha = 0.05\n",
    "#assign the x and y variables from the train dataset\n",
    "x = train.latitude\n",
    "y = train.logerror\n",
    "#run a pearson r correlation test\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "#output the results of the test\n",
    "print(f'Correlation Coefficient = {corr} P-Value = {p}')\n",
    "print('-----------------------------------')\n",
    "if p < alpha:\n",
    "    print('We reject the null hypothesis')\n",
    "else:\n",
    "    print('We do not reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89deb850",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af6871d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=\"longitude\", y=\"logerror\", data=train)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"log error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#assign an alpha value\n",
    "alpha = 0.05\n",
    "#assign the x and y variables from the train dataset\n",
    "x = train.longitude\n",
    "y = train.logerror\n",
    "#run a pearson r correlation test\n",
    "corr, p = stats.pearsonr(x, y)\n",
    "#output the results of the test\n",
    "print(f'Correlation Coefficient = {corr} P-Value = {p}')\n",
    "print('-----------------------------------')\n",
    "if p < alpha:\n",
    "    print('We reject the null hypothesis')\n",
    "else:\n",
    "    print('We do not reject the null hypothesis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5fd303",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ff564",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c9fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = acquire_zillow()\n",
    "df = single_unit_properties(df)\n",
    "remove_cols = ['buildingqualitytypeid', 'heatingorsystemtypeid', 'propertyzoningdesc', 'heatingorsystemdesc']\n",
    "df = data_prep(df, cols_to_remove=remove_cols, prop_required_column=.5, prop_required_row=.75)\n",
    "df1, df2, df3 = split_by_region(df)\n",
    "df1['dist_lat'] = df1.latitude - 34012355\n",
    "df1['dist_long'] = df1.longitude - (-118498665)\n",
    "df2['dist_lat'] = df2.latitude - 33640954\n",
    "df2['dist_long'] = df2.longitude - (-117978893)\n",
    "df3['dist_lat'] = df3.latitude - 34267111\n",
    "df3['dist_long'] = df3.longitude - (-119278788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train1, validate1, test1 = split_data(df1)\n",
    "train2, validate2, test2 = split_data(df2)\n",
    "train3, validate3, test3 = split_data(df3)\n",
    "\n",
    "out_cols = ['bedroomcnt', 'bathroomcnt', 'calculatedfinishedsquarefeet',]\n",
    "train1 = remove_outliers(train1, 1.5, out_cols)\n",
    "train2 = remove_outliers(train2, 1.5, out_cols)\n",
    "train3 = remove_outliers(train3, 1.5, out_cols)\n",
    "\n",
    "cluster_df1_train = cluster_model_prep(train1)\n",
    "cluster_df1_val = cluster_model_prep(validate1)\n",
    "cluster_df1_test = cluster_model_prep(test1)\n",
    "\n",
    "cluster_df2_train = cluster_model_prep(train2)\n",
    "cluster_df2_val = cluster_model_prep(validate2)\n",
    "cluster_df2_test = cluster_model_prep(test2)\n",
    "\n",
    "cluster_df3_train = cluster_model_prep(train3)\n",
    "cluster_df3_val = cluster_model_prep(validate3)\n",
    "cluster_df3_test = cluster_model_prep(test3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9467428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b45a64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a2b0a07",
   "metadata": {},
   "source": [
    "Cluster 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f764c87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1\n",
    "x = cluster_df1_train[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train1['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df1_val[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate1['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df1_test[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test1['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "\n",
    "\n",
    "#df2\n",
    "x = cluster_df2_train[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train2['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df2_val[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate2['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df2_test[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test2['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "\n",
    "#df3\n",
    "x = cluster_df3_train[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train3['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df3_val[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate3['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df3_test[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "kmeans = KMeans(n_clusters=3, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test3['bedbathsqft_cluster'] = kmeans.predict(x)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc2aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=train1, y='logerror', x='calculatedfinishedsquarefeet', hue='bedbathsqft_cluster')\n",
    "\n",
    "sns.relplot(data=train2, y='logerror', x='calculatedfinishedsquarefeet', hue='bedbathsqft_cluster')\n",
    "\n",
    "sns.relplot(data=train3, y='logerror', x='calculatedfinishedsquarefeet', hue='bedbathsqft_cluster')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4852a48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_1 = train1[train1['bedbathsqft_cluster']== 0]\n",
    "clust_2 = train1[train1['bedbathsqft_cluster']== 1]\n",
    "clust_3 = train1[train1['bedbathsqft_cluster']== 2]\n",
    "print('df1 log error means:')\n",
    "print(clust_1.logerror.mean(), clust_2.logerror.mean(), clust_3.logerror.mean())\n",
    "\n",
    "clust_4 = train2[train2['bedbathsqft_cluster']== 0]\n",
    "clust_5 = train2[train2['bedbathsqft_cluster']== 1]\n",
    "clust_6 = train2[train2['bedbathsqft_cluster']== 2]\n",
    "print('df2 log error means:')\n",
    "print(clust_4.logerror.mean(), clust_5.logerror.mean(), clust_6.logerror.mean())\n",
    "\n",
    "clust_7 = train3[train3['bedbathsqft_cluster']== 0]\n",
    "clust_8 = train3[train3['bedbathsqft_cluster']== 1]\n",
    "clust_9 = train3[train3['bedbathsqft_cluster']== 2]\n",
    "print('df3 log error means:')\n",
    "print(clust_7.logerror.mean(), clust_8.logerror.mean(), clust_9.logerror.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15daeb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86439e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3363f4d8",
   "metadata": {},
   "source": [
    "Cluster 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059a8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1\n",
    "x = cluster_df1_train[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train1['latlong_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df1_val[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate1['latlong_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df1_test[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test1['latlong_cluster'] = kmeans.predict(x)\n",
    "\n",
    "\n",
    "#df2\n",
    "x = cluster_df2_train[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train2['latlong_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df2_val[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate2['latlong_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df2_test[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test2['latlong_cluster'] = kmeans.predict(x)\n",
    "\n",
    "\n",
    "#df3\n",
    "x = cluster_df3_train[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train3['latlong_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df3_val[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate3['latlong_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df3_test[['latitude', 'longitude']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test3['latlong_cluster'] = kmeans.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9802e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=train1, y='logerror', x='latitude', hue='latlong_cluster')\n",
    "sns.relplot(data=train1, y='logerror', x='longitude', hue='latlong_cluster')\n",
    "\n",
    "sns.relplot(data=train2, y='logerror', x='latitude', hue='latlong_cluster')\n",
    "sns.relplot(data=train2, y='logerror', x='longitude', hue='latlong_cluster')\n",
    "\n",
    "sns.relplot(data=train3, y='logerror', x='latitude', hue='latlong_cluster')\n",
    "sns.relplot(data=train3, y='logerror', x='longitude', hue='latlong_cluster')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3edde8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clust_1 = train1[train1['latlong_cluster']== 0]\n",
    "clust_2 = train1[train1['latlong_cluster']== 1]\n",
    "clust_3 = train1[train1['latlong_cluster']== 2]\n",
    "clust_4 = train1[train1['latlong_cluster']== 3]\n",
    "print('df1 log error means:')\n",
    "print(clust_1.logerror.mean(), clust_2.logerror.mean(), clust_3.logerror.mean(), clust_4.logerror.mean())\n",
    "\n",
    "clust_5 = train2[train2['latlong_cluster']== 0]\n",
    "clust_6 = train2[train2['latlong_cluster']== 1]\n",
    "clust_7 = train2[train2['latlong_cluster']== 2]\n",
    "clust_8 = train2[train2['latlong_cluster']== 3]\n",
    "print('df2 log error means:')\n",
    "print(clust_5.logerror.mean(), clust_6.logerror.mean(), clust_7.logerror.mean(), clust_8.logerror.mean())\n",
    "\n",
    "clust_9 = train3[train3['latlong_cluster']== 0]\n",
    "clust_10 = train3[train3['latlong_cluster']== 1]\n",
    "clust_11 = train3[train3['latlong_cluster']== 2]\n",
    "clust_12 = train3[train3['latlong_cluster']== 3]\n",
    "print('df3 log error means:')\n",
    "print(clust_9.logerror.mean(), clust_10.logerror.mean(), clust_11.logerror.mean(), clust_12.logerror.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293444d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c2acbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94d01345",
   "metadata": {},
   "source": [
    "cluster 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbcee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df1\n",
    "x = cluster_df1_train[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train1['dist_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df1_val[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate1['dist_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df1_test[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test1['dist_cluster'] = kmeans.predict(x)\n",
    "\n",
    "#df2\n",
    "x = cluster_df2_train[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train2['dist_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df2_val[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate2['dist_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df2_test[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test2['dist_cluster'] = kmeans.predict(x)\n",
    "\n",
    "#df3\n",
    "x = cluster_df3_train[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "train3['dist_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df3_val[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "validate3['dist_cluster'] = kmeans.predict(x)\n",
    "\n",
    "x = cluster_df3_test[['dist_lat', 'dist_long']]\n",
    "kmeans = KMeans(n_clusters=4, random_state=123)\n",
    "kmeans.fit(x)\n",
    "test3['dist_cluster'] = kmeans.predict(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25f3da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.relplot(data=train1, y='logerror', x='dist_lat', hue='dist_cluster')\n",
    "sns.relplot(data=train1, y='logerror', x='dist_long', hue='dist_cluster')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad9b84b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562035a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f5330c34",
   "metadata": {},
   "source": [
    "Prep for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83946025",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrmodel_train1_df = lr_model_prep(cluster_df1_train, train1)\n",
    "lrmodel_val1_df = lr_model_prep(cluster_df1_val, validate1)\n",
    "lrmodel_test1_df = lr_model_prep(cluster_df1_test, test1)\n",
    "\n",
    "lrmodel_train2_df = lr_model_prep(cluster_df2_train, train2)\n",
    "lrmodel_val2_df = lr_model_prep(cluster_df2_val, validate2)\n",
    "lrmodel_test2_df = lr_model_prep(cluster_df2_test, test2)\n",
    "\n",
    "lrmodel_train3_df = lr_model_prep(cluster_df3_train, train3)\n",
    "lrmodel_val3_df = lr_model_prep(cluster_df3_val, validate3)\n",
    "lrmodel_test3_df = lr_model_prep(cluster_df3_test, test3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38efce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53e0c440",
   "metadata": {},
   "source": [
    "Linear Regression modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3289bb",
   "metadata": {},
   "source": [
    "Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17791052",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = lrmodel_train1_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "y_train_features = train1[['logerror']]\n",
    "x_validate_features = lrmodel_val1_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "y_validate_features = validate1[['logerror']]\n",
    "\n",
    "m1d1_predictions = pd.DataFrame({'actual': validate1.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m1d1_predictions['baseline'] = validate1.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m1d1_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_1(y_predicted):\n",
    "    return mean_squared_error(m1d1_predictions.actual, y_predicted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_features = lrmodel_train2_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "y_train_features = train2[['logerror']]\n",
    "x_validate_features = lrmodel_val2_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "y_validate_features = validate2[['logerror']]\n",
    "\n",
    "m1d2_predictions = pd.DataFrame({'actual': validate2.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m1d2_predictions['baseline'] = validate2.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m1d2_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_2(y_predicted):\n",
    "    return mean_squared_error(m1d2_predictions.actual, y_predicted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_features = lrmodel_train3_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "y_train_features = train3[['logerror']]\n",
    "x_validate_features = lrmodel_val3_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet']]\n",
    "y_validate_features = validate3[['logerror']]\n",
    "\n",
    "m1d3_predictions = pd.DataFrame({'actual': validate3.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m1d3_predictions['baseline'] = validate3.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m1d3_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_3(y_predicted):\n",
    "    return mean_squared_error(m1d3_predictions.actual, y_predicted)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d016565",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 1 with the Los Angeles County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m1d1_predictions.apply(calculate_mse_1).apply(math.sqrt))\n",
    "print('==================================================')\n",
    "print('')\n",
    "print('Model 1 with the Orange County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m1d2_predictions.apply(calculate_mse_2).apply(math.sqrt))\n",
    "print('==================================================')\n",
    "print('')\n",
    "print('Model 1 with the Ventura County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m1d3_predictions.apply(calculate_mse_3).apply(math.sqrt))\n",
    "print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de812020",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ddc05e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "398eb885",
   "metadata": {},
   "source": [
    "Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7e7bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = lrmodel_train1_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'latitude', 'longitude']]\n",
    "y_train_features = train1[['logerror']]\n",
    "x_validate_features = lrmodel_val1_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'latitude', 'longitude']]\n",
    "y_validate_features = validate1[['logerror']]\n",
    "\n",
    "m2d1_predictions = pd.DataFrame({'actual': validate1.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m2d1_predictions['baseline'] = validate1.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m2d1_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_4(y_predicted):\n",
    "    return mean_squared_error(m2d1_predictions.actual, y_predicted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_features = lrmodel_train2_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'latitude', 'longitude']]\n",
    "y_train_features = train2[['logerror']]\n",
    "x_validate_features = lrmodel_val2_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'latitude', 'longitude']]\n",
    "y_validate_features = validate2[['logerror']]\n",
    "\n",
    "m2d2_predictions = pd.DataFrame({'actual': validate2.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m2d2_predictions['baseline'] = validate2.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m2d2_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_5(y_predicted):\n",
    "    return mean_squared_error(m2d2_predictions.actual, y_predicted)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x_train_features = lrmodel_train3_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'latitude', 'longitude']]\n",
    "y_train_features = train3[['logerror']]\n",
    "x_validate_features = lrmodel_val3_df[['bathroomcnt', 'bedroomcnt', 'calculatedfinishedsquarefeet', 'latitude', 'longitude']]\n",
    "y_validate_features = validate3[['logerror']]\n",
    "\n",
    "m2d3_predictions = pd.DataFrame({'actual': validate3.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m2d3_predictions['baseline'] = validate3.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m2d3_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_6(y_predicted):\n",
    "    return mean_squared_error(m2d3_predictions.actual, y_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d459d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 2 with the Los Angeles County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m2d1_predictions.apply(calculate_mse_4).apply(math.sqrt))\n",
    "print('==================================================')\n",
    "print('')\n",
    "print('Model 2 with the Orange County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m2d2_predictions.apply(calculate_mse_5).apply(math.sqrt))\n",
    "print('==================================================')\n",
    "print('')\n",
    "print('Model 2 with the Ventura County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m2d3_predictions.apply(calculate_mse_6).apply(math.sqrt))\n",
    "print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71d04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebff8ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "800417a3",
   "metadata": {},
   "source": [
    "Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab62d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = lrmodel_train1_df[['bedbathsqft_cluster_1', 'bedbathsqft_cluster_2', 'latlong_cluster_1', 'latlong_cluster_2',\n",
    "       'latlong_cluster_3']]\n",
    "y_train_features = train1[['logerror']]\n",
    "x_validate_features = lrmodel_val1_df[['bedbathsqft_cluster_1', 'bedbathsqft_cluster_2', 'latlong_cluster_1', 'latlong_cluster_2',\n",
    "       'latlong_cluster_3']]\n",
    "y_validate_features = validate1[['logerror']]\n",
    "\n",
    "m4d1_predictions = pd.DataFrame({'actual': validate1.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m4d1_predictions['baseline'] = validate1.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=5, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m4d1_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_10(y_predicted):\n",
    "    return mean_squared_error(m4d1_predictions.actual, y_predicted)\n",
    "##########################\n",
    "x_train_features = lrmodel_train2_df[['bedbathsqft_cluster_1', 'bedbathsqft_cluster_2', 'latlong_cluster_1', 'latlong_cluster_2',\n",
    "       'latlong_cluster_3']]\n",
    "y_train_features = train2[['logerror']]\n",
    "x_validate_features = lrmodel_val2_df[['bedbathsqft_cluster_1', 'bedbathsqft_cluster_2', 'latlong_cluster_1', 'latlong_cluster_2',\n",
    "       'latlong_cluster_3']]\n",
    "y_validate_features = validate2[['logerror']]\n",
    "\n",
    "m4d2_predictions = pd.DataFrame({'actual': validate2.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m4d2_predictions['baseline'] = validate2.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=5, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m4d2_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_11(y_predicted):\n",
    "    return mean_squared_error(m4d2_predictions.actual, y_predicted)\n",
    "###########################\n",
    "x_train_features = lrmodel_train3_df[['bedbathsqft_cluster_1', 'bedbathsqft_cluster_2', 'latlong_cluster_1', 'latlong_cluster_2',\n",
    "       'latlong_cluster_3']]\n",
    "y_train_features = train3[['logerror']]\n",
    "x_validate_features = lrmodel_val3_df[['bedbathsqft_cluster_1', 'bedbathsqft_cluster_2', 'latlong_cluster_1', 'latlong_cluster_2',\n",
    "       'latlong_cluster_3']]\n",
    "y_validate_features = validate3[['logerror']]\n",
    "\n",
    "m4d3_predictions = pd.DataFrame({'actual': validate3.logerror})\n",
    "#add a baseline column to the predictions dataframe with median home price\n",
    "m4d3_predictions['baseline'] = validate3.logerror.mean()\n",
    "#create the polynomial model object\n",
    "poly = PolynomialFeatures(degree=5, include_bias=False, interaction_only=True)\n",
    "#fit the polynomial model object with the scaled x_train variables\n",
    "poly.fit(x_train_features)\n",
    "#create a dataframe with the results of fitting the polynomial object\n",
    "x_train_poly = pd.DataFrame(\n",
    "    poly.transform(x_train_features),\n",
    "    columns=poly.get_feature_names(x_train_features.columns),\n",
    "    index=x_train_features.index,\n",
    ")\n",
    "#create a linear regression model object\n",
    "lm = LinearRegression()\n",
    "#fit the linear regression model with the above created polynomial dataframe\n",
    "lm.fit(x_train_poly, y_train_features)\n",
    "#make predictions on the scaled validate data\n",
    "x_validate_poly = poly.transform(x_validate_features)\n",
    "#create a column in the predictions dataframe with the resulting model predictions\n",
    "m4d3_predictions['polynomial only interaction'] = lm.predict(x_validate_poly)\n",
    "#calculate the mean squared error for the baseline and model predictions against the actual home prices\n",
    "def calculate_mse_12(y_predicted):\n",
    "    return mean_squared_error(m4d3_predictions.actual, y_predicted)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14515b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Model 4 with the Los Angeles County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m4d1_predictions.apply(calculate_mse_10).apply(math.sqrt))\n",
    "print('==================================================')\n",
    "print('')\n",
    "print('Model 4 with the Orange County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m4d2_predictions.apply(calculate_mse_11).apply(math.sqrt))\n",
    "print('==================================================')\n",
    "print('')\n",
    "print('Model 4 with the Ventura County, CA dataset:')\n",
    "print('--------------------------------------------------')\n",
    "print(m4d3_predictions.apply(calculate_mse_12).apply(math.sqrt))\n",
    "print('==================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d989166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d458a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7844770",
   "metadata": {},
   "source": [
    "model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ba6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
